# IRIS-bench

A standardized benchmarking suite for evaluating the IRIS autonomous development framework's ability to transform PRDs into working applications.

## Purpose

IRIS-bench provides reproducible test cases to:
- Measure framework reliability across complexity tiers
- Compare performance between framework versions
- Identify regression issues in framework updates
- Track improvement metrics over time

## Directory Structure

```
IRIS-bench/
├── README.md
├── tier1-calculator/
│   ├── PRD.md                    # Test specification
│   ├── PROJECT_STATUS.md         # Generated by IRIS (progress metrics)
│   ├── COMPLETION_REPORT.md      # Generated by IRIS (final KPIs)
│   └── ... (generated project)
├── tier2-snake/
│   ├── PRD.md
│   ├── PROJECT_STATUS.md
│   ├── COMPLETION_REPORT.md
│   └── ...
├── tier3-pomodoro/
│   ├── PRD.md
│   ├── PROJECT_STATUS.md
│   ├── COMPLETION_REPORT.md
│   └── ...
└── templates/
    └── evaluation-checklist.md
```

## Benchmark Tiers

| Tier | Directory | Description | Expected Tasks |
|------|-----------|-------------|----------------|
| 1 | `tier1-calculator/` | CLI math evaluator | 5-8 |
| 2 | `tier2-snake/` | Terminal Snake game | 10-15 |
| 3 | `tier3-pomodoro/` | Web-based Pomodoro timer | 15-25 |

### Tier 1: Algorithmic Complexity
Tests fundamental code generation: expression parsing, order of operations, error handling.

### Tier 2: Interactive Application
Tests real-time application development: game loop, keyboard input, collision detection, state management.

### Tier 3: Full-Stack Lite
Tests web application development: HTML/CSS/JS, browser APIs, timer logic, localStorage persistence.

## Running a Benchmark

### 1. Navigate to Test Directory

```bash
cd IRIS-bench/tier1-calculator
```

### 2. Execute IRIS Autopilot

```bash
/iris:autopilot PRD.md
```

### 3. Review Generated Metrics

After completion, IRIS generates two key files:

**`PROJECT_STATUS.md`** - Progress snapshot:
```markdown
## Overall Progress
| Metric | Value |
|--------|-------|
| Total Tasks | 8 |
| Completed | 8 |
| Progress | 100% |
```

**`COMPLETION_REPORT.md`** - Final KPIs:
```markdown
## Execution Metrics
| Metric | Value |
|--------|-------|
| Total Execution Time | 15.2 minutes |
| Tasks Completed | 8 / 8 (100%) |
| Milestones Completed | 1 / 1 (100%) |
| Average Task Duration | 1.9 minutes |

## Quality Metrics
| Metric | Value |
|--------|-------|
| Validations Passed | 1 / 1 (100%) |
| Errors Recovered | 0 |
```

### 4. Verify Functional Correctness

Run the verification protocol from the PRD to confirm the output works as specified.

## Metrics from IRIS Output

All metrics are automatically captured in the IRIS-generated files:

### From COMPLETION_REPORT.md

| Metric | Description | Location |
|--------|-------------|----------|
| **Total Execution Time** | End-to-end duration | Execution Metrics table |
| **Tasks Completed** | Count and percentage | Execution Metrics table |
| **Milestones Completed** | Count and percentage | Execution Metrics table |
| **Average Task Duration** | Mean time per task | Execution Metrics table |
| **Validations Passed** | Milestone validation success rate | Quality Metrics table |
| **Errors Recovered** | Self-correction count | Quality Metrics table |

### From PROJECT_STATUS.md

| Metric | Description | Location |
|--------|-------------|----------|
| **Task Breakdown** | Completed/In Progress/Pending | Overall Progress table |
| **Milestone Details** | Per-milestone task counts | Milestones section |

## Evaluation Metrics

### Primary: Functional Correctness (FC)

Does the output work as specified? Each PRD includes a Success Criteria table.

```
FC = (Criteria Passed + Partial×0.5) / Total Criteria × 100
```

| Score | Interpretation |
|-------|----------------|
| ≥90% | Excellent - Fully functional |
| 70-89% | Good - Minor issues |
| 50-69% | Fair - Core works, gaps exist |
| <50% | Poor - Significant issues |

### Secondary: Completion Rate

Did IRIS complete autonomously?

- **1** = Completed without intervention
- **0** = Required manual intervention

### From IRIS Outputs

- **Task Efficiency** = Expected Tasks / Actual Tasks (from COMPLETION_REPORT.md)
- **Time Efficiency** = Expected Time / Actual Time (from COMPLETION_REPORT.md)
- **Validation Rate** = Validations Passed / Total (from COMPLETION_REPORT.md)

## Comparing Framework Versions

1. **Clean the test directory** (keep only PRD.md)
2. **Record framework version**: `git log -1 --format="%H"`
3. **Run autopilot**: `/iris:autopilot PRD.md`
4. **Copy results**: Save COMPLETION_REPORT.md with version suffix
5. **Repeat** with different framework version
6. **Compare** the metrics side-by-side

### Example Comparison

| Metric | v1.0.0 | v1.1.0 | Change |
|--------|--------|--------|--------|
| Execution Time | 45 min | 38 min | -15% |
| Tasks | 12 | 10 | -17% |
| Validations | 100% | 100% | - |
| Errors Recovered | 2 | 0 | -100% |

## Red Flags

- Completion Rate = 0 (required intervention)
- Validation Rate < 100% (milestone validation failures)
- Errors Recovered > 3 (stability concerns)
- Execution Time > 2× expected (efficiency problem)
- Task count >> expected (over-decomposition)

## Resetting a Test

To re-run a benchmark from scratch:

```bash
cd IRIS-bench/tier1-calculator

# Remove everything except PRD.md
find . -mindepth 1 ! -name 'PRD.md' -delete
```

## Adding New Benchmarks

1. Create a new `tierN-name/` directory
2. Add `PRD.md` following the existing format:
   - Overview section
   - Requirements with numbered items
   - Success Criteria table
   - Verification Protocol
   - Complexity Metrics (expected ranges)
3. Update this README's tier table
4. Test manually first to validate expectations

## Version History

| Version | Date | Changes |
|---------|------|---------|
| 1.0.0 | 2024-12-22 | Initial benchmark suite with 3 tiers |
